{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652a5791-3076-492b-b379-1194f6f4b523",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ff726d7-e2cc-4873-9179-c4fbe87f3dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/HT/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-03-16 14:34:59.244886: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-16 14:35:00.216232: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib/python3.8/site-packages/torch/lib:/opt/conda/lib/python3.8/site-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-03-16 14:35:00.216434: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib/python3.8/site-packages/torch/lib:/opt/conda/lib/python3.8/site-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-03-16 14:35:00.216449: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pathlib\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f8d2446-a31a-4919-9d60-492284db26de",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc2ef4cc-f68a-4cc2-877b-8451a35c7298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# nlp = pipeline(\"ner\", model=\"Jean-Baptiste/roberta-large-ner-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423a6be6-4ca3-41b6-9bf5-cc1c6020349d",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a56e035c-714e-44ec-ade1-c457eedb22ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "east_df = pd.read_csv(\"../data/structured/TEXT/east.csv\", low_memory=False)\n",
    "west_df = pd.read_csv(\"../data/structured/TEXT/west.csv\", low_memory=False)\n",
    "north_df = pd.read_csv(\"../data/structured/TEXT/north.csv\", low_memory=False)\n",
    "south_df = pd.read_csv(\"../data/structured/TEXT/south.csv\", low_memory=False)\n",
    "central_df = pd.read_csv(\"../data/structured/TEXT/central.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cce4f3d2-7876-4543-9e40-be4732c2cb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'TEXT', 'PHONES', 'CITY', 'IMAGES', 'DEMO', 'VENDOR'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "east_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e164640b-1a83-4f33-ba45-d20b0a8f50a2",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53a9f625-e38b-4f4e-ac9b-8e2a6b22dade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)95525/.gitattributes: 100%|██████████| 1.17k/1.17k [00:00<00:00, 138kB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 19.5kB/s]\n",
      "Downloading (…)ed27695525/README.md: 100%|██████████| 3.96k/3.96k [00:00<00:00, 867kB/s]\n",
      "Downloading (…)27695525/config.json: 100%|██████████| 718/718 [00:00<00:00, 193kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 117/117 [00:00<00:00, 25.6kB/s]\n",
      "Downloading (…)aluation_results.csv: 100%|██████████| 659/659 [00:00<00:00, 34.4kB/s]\n",
      "Downloading (…)d27695525/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.13MB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 499M/499M [00:12<00:00, 39.7MB/s] \n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 6.35kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 45.0kB/s]\n",
      "Downloading (…)95525/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 2.26MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 354/354 [00:00<00:00, 87.4kB/s]\n",
      "Downloading (…)d27695525/vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 1.62MB/s]\n",
      "Downloading (…)7695525/modules.json: 100%|██████████| 229/229 [00:00<00:00, 55.2kB/s]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"AnnaWegmann/Style-Embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89852ee1-2a91-4753-a773-942a6c83b7d1",
   "metadata": {},
   "source": [
    "# Calculating similarity through existing style embedding models on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b998c18b-cdc5-450d-87d8-4147c3a07515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_heatmap(data_df, vendor_id):\n",
    "    phones = set(data_df[data_df.VENDOR==vendor_id]['PHONES'].to_list())\n",
    "    phones = [eval(phone) for phone in phones]\n",
    "    phones = [item for sublist in phones for item in sublist]\n",
    "    print(\"All phone numbers:\", phones)\n",
    "        \n",
    "    sent_list = set(data_df[data_df.VENDOR==vendor_id]['TEXT'].to_list())\n",
    "    outer_list = []\n",
    "    for index1, sent1 in enumerate(sent_list):\n",
    "        inner_list = []\n",
    "        for index2, sent2 in enumerate(sent_list):\n",
    "            emb1 = model.encode(sent1)\n",
    "            emb2 = model.encode(sent2)\n",
    "            inner_list.append(util.cos_sim(emb1, emb2).cpu().detach().numpy()[0][0])\n",
    "        outer_list.append(inner_list)\n",
    "\n",
    "    fig = px.imshow(outer_list, text_auto=True, aspect=\"auto\")\n",
    "    fig.show('iframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2194363-6d94-46e2-b828-094ef3663833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All phone numbers: ['814-602-6807']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_5.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_heatmap(west_df, 63072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da10476c-f529-4b5d-b73e-cdf2e188ded9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All phone numbers: ['8145049661']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_6.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_heatmap(west_df, 63076)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f34547f-87d7-4e7e-bae6-3c53e2527e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All phone numbers: ['814 920 8940', '814 923 6153', '814 920 8940']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_7.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_heatmap(west_df, 63081)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cf4c4b-9d0f-4aa2-80aa-8659e6c0e510",
   "metadata": {},
   "source": [
    "# Calculating similarity through existing style embedding models on processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e00669e-477b-4c6c-ada0-19457ec28c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "extensionsToCheck = ('.com', '.co', '.in', '.net', '.to', '.org', '.us', '.edu', '.gov', '.int')\n",
    "def anonymize_links(sent):\n",
    "    # Removing all the links\n",
    "    sent = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', \"<LINK>\", sent)\n",
    "    sent = sent.split(\" \")\n",
    "    sent = [\"<LINK>\" if text.endswith(extensionsToCheck) else text for text in sent]\n",
    "    return \" \".join(sent)\n",
    "\n",
    "def anonymize_age(sent):\n",
    "    # Removing the age and post id from the advertisement\n",
    "    ages = re.findall(r'age:\\s*(\\d+)', sent)\n",
    "    if ages:\n",
    "        for age in ages:\n",
    "            # Removing age under the age: section\n",
    "            sent = sent.replace(\"age: \" + age, \"age: <AGE>\")\n",
    "            # Removing age everywhere else\n",
    "            sent = sent.replace(\" \" + age + \" \", \"<AGE>\")\n",
    "    return sent\n",
    "\n",
    "def anonymize_postid(sent):\n",
    "    # Removing the Post ID from the advertisements\n",
    "    id_ = re.findall(r'Post ID:\\s*(\\d+)', sent)\n",
    "    if id_:\n",
    "        sent = sent.replace(\"Post ID: \" + id_[0], \"Post ID: <POST_ID>\")\n",
    "    return sent\n",
    "\n",
    "\n",
    "num_order = r'[0-9]'\n",
    "def find_location(data_df):\n",
    "    location_dict = {}\n",
    "    all_vendors = list(data_df.VENDOR.unique())\n",
    "    pbar = tqdm(total=len(all_vendors))\n",
    "    \n",
    "    for vendor in all_vendors:\n",
    "        location_list = []\n",
    "        data = data_df[data_df.VENDOR == vendor]\n",
    "        text = data.TEXT.to_list()\n",
    "        for sent in text:\n",
    "            doc = nlp(sent)\n",
    "            for entity in doc.ents:\n",
    "                # Checking for all Countries, cities, states, Non-GPE locations, mountain ranges, bodies of water, Buildings, airports, highways, bridges, etc\n",
    "                if entity.label_ in {\"GPE\", \"LOC\", \"FAC\"}:\n",
    "                    # Removing numbers from the string\n",
    "                    location = re.sub(num_order, '', entity.text)\n",
    "                    # Removing / , and . \n",
    "                    location = location.replace(\"/\", \"#\").replace(\",\", \"#\").replace(\".\", \"#\").split(\"#\")\n",
    "                    for loc in location:\n",
    "                        if len(loc) > 1 and loc not in location_list:\n",
    "                            location_list.append(loc.strip())\n",
    "        location_dict[vendor] = set(location_list)\n",
    "        \n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    return location_dict\n",
    "\n",
    "num_order = r'[0-9]'\n",
    "def find_names(data_df):\n",
    "    names_dict = {}\n",
    "    all_vendors = list(data_df.VENDOR.unique())\n",
    "    pbar = tqdm(total=len(all_vendors))\n",
    "    \n",
    "    for vendor in all_vendors:\n",
    "        names_list = []\n",
    "        data = data_df[data_df.VENDOR == vendor]\n",
    "        text = data.TEXT.to_list()\n",
    "        for sent in text:\n",
    "            doc = nlp(sent)\n",
    "            for entity in doc.ents:\n",
    "                # Checking for all Countries, cities, states, Non-GPE locations, mountain ranges, bodies of water, Buildings, airports, highways, bridges, etc\n",
    "                if entity.label_ in {\"PERSON\"}:\n",
    "                    # Removing numbers from the string\n",
    "                    names = re.sub(num_order, '', entity.text)\n",
    "                    # Removing / , and . \n",
    "                    names = names.replace(\"/\", \"#\").replace(\",\", \"#\").replace(\".\", \"#\").split(\"#\")\n",
    "                    for name in names:\n",
    "                        # Removing all the wrongly identified names that have two or more tokens\n",
    "                        if  0 < len(name.split(\" \")) < 2 and name not in names_list:\n",
    "                            # Removing all the special symbols from the names\n",
    "                            name = re.sub(\"[^A-Z]\", \"\", name, 0, re.IGNORECASE)\n",
    "                            # Checking if the name is atleast 3 characters long \n",
    "                            if len(name) >= 3:\n",
    "                                names_list.append(name.strip())\n",
    "        if len(names_list) != 0:\n",
    "            names_dict[vendor] = set(names_list)\n",
    "        \n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    return names_dict\n",
    "\n",
    "def find_email(data_df):\n",
    "    email_dict = {}\n",
    "    all_vendors = list(data_df.VENDOR.unique())\n",
    "    pbar = tqdm(total=len(all_vendors))\n",
    "    \n",
    "    for vendor in all_vendors:\n",
    "        email_list = []\n",
    "        data = data_df[data_df.VENDOR == vendor]\n",
    "        text = data.TEXT.to_list()\n",
    "        for sent in text:\n",
    "            sent = re.findall(\"([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)\", sent)\n",
    "            email_list.append(sent)\n",
    "        \n",
    "        email_list = [item for sublist in email_list for item in sublist]\n",
    "        email_dict[vendor] = set(email_list)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "        \n",
    "    return email_dict\n",
    "\n",
    "def anonymize_locations(sent, all_loc_dict):\n",
    "    for location in all_loc_dict.keys():\n",
    "        sent = re.sub(r'([^\\w\\s])', r' \\1 ', sent)\n",
    "        if \" \" + location + \" \" in sent:\n",
    "            sent = sent.replace(\" \" + location + \" \", \" \" + all_loc_dict[location] + \" \")\n",
    "    return sent\n",
    "\n",
    "def anonymize_emails(sent, all_email_dict):\n",
    "    for email in all_email_dict.keys():\n",
    "        if email in sent:\n",
    "            sent = sent.replace(email, all_email_dict[email])\n",
    "    return sent\n",
    "\n",
    "def anonymize_names(sent, all_name_dict):\n",
    "    for name in all_name_dict.keys():\n",
    "        if name in sent:\n",
    "            \n",
    "            sent = sent.replace(name, all_name_dict[name])\n",
    "    return sent\n",
    "\n",
    "def anonymize_numbers(sent):\n",
    "    sent = ''.join(i if not i.isdigit() else \"N\" for i in sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08199856-d1a1-46a5-aa2c-4f88afe7fbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor data_df in [north_df, south_df, west_df, east_df, central_df]:\\n    print(\"Demography:\", list(data_df.DEMO.unique())[0])\\n    location_dict = find_email(data_df)\\n    \\n    with open(os.path.join(os.getcwd(), \"../pickled/dictionaries\",\\'email_dict_\\' + list(data_df.DEMO.unique())[0] + \\'.pickle\\'), \\'wb\\') as handle:\\n        pickle.dump(location_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment it to fetch a dictionary with anonymized names \n",
    "\"\"\"\n",
    "for data_df in [north_df, south_df, west_df, east_df, central_df]:\n",
    "    print(\"Demography:\", list(data_df.DEMO.unique())[0])\n",
    "    location_dict = find_email(data_df)\n",
    "    \n",
    "    with open(os.path.join(os.getcwd(), \"../pickled/dictionaries\",'email_dict_' + list(data_df.DEMO.unique())[0] + '.pickle'), 'wb') as handle:\n",
    "        pickle.dump(location_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cc628af-41ba-45b3-a84d-125db3aa65f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pickled dictionary\n",
    "\n",
    "with open('../pickled/dictionaries/loc_dict_east.pickle', 'rb') as handle:\n",
    "    east_loc_dict = pickle.load(handle)\n",
    "    \n",
    "with open('../pickled/dictionaries/loc_dict_west.pickle', 'rb') as handle:\n",
    "    west_loc_dict = pickle.load(handle)\n",
    "    \n",
    "with open('../pickled/dictionaries/loc_dict_north.pickle', 'rb') as handle:\n",
    "    north_loc_dict = pickle.load(handle)\n",
    "    \n",
    "with open('../pickled/dictionaries/loc_dict_south.pickle', 'rb') as handle:\n",
    "    south_loc_dict = pickle.load(handle)\n",
    "    \n",
    "with open('../pickled/dictionaries/loc_dict_central.pickle', 'rb') as handle:\n",
    "    central_loc_dict = pickle.load(handle)\n",
    "    \n",
    "with open('../pickled/dictionaries/names_dict_central.pickle', 'rb') as handle:\n",
    "    central_name_dict = pickle.load(handle)\n",
    "    \n",
    "with open('../pickled/dictionaries/names_dict_east.pickle', 'rb') as handle:\n",
    "    east_name_dict = pickle.load(handle)\n",
    "    \n",
    "with open('../pickled/dictionaries/names_dict_west.pickle', 'rb') as handle:\n",
    "    west_name_dict = pickle.load(handle)\n",
    "    \n",
    "with open('../pickled/dictionaries/names_dict_north.pickle', 'rb') as handle:\n",
    "    north_name_dict = pickle.load(handle)\n",
    "    \n",
    "with open('../pickled/dictionaries/names_dict_south.pickle', 'rb') as handle:\n",
    "    south_name_dict = pickle.load(handle)\n",
    "    \n",
    "with open('../pickled/dictionaries/email_dict_central.pickle', 'rb') as handle:\n",
    "    central_email_dict = pickle.load(handle)\n",
    "    \n",
    "with open('../pickled/dictionaries/email_dict_east.pickle', 'rb') as handle:\n",
    "    east_email_dict = pickle.load(handle)\n",
    "    \n",
    "with open('../pickled/dictionaries/email_dict_west.pickle', 'rb') as handle:\n",
    "    west_email_dict = pickle.load(handle)\n",
    "    \n",
    "with open('../pickled/dictionaries/email_dict_north.pickle', 'rb') as handle:\n",
    "    north_email_dict = pickle.load(handle)\n",
    "    \n",
    "with open('../pickled/dictionaries/email_dict_south.pickle', 'rb') as handle:\n",
    "    south_email_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41fb992c-d240-41ad-bcad-eebae6be278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting IDs for all the names\n",
    "names = list(set([item for sublist in list(east_name_dict.values()) for item in sublist])) + list(set([item for sublist in list(west_name_dict.values()) for item in sublist])) + list(set([item for sublist in list(north_name_dict.values()) for item in sublist])) + list(set([item for sublist in list(south_name_dict.values()) for item in sublist])) + list(set([item for sublist in list(central_name_dict.values()) for item in sublist]))\n",
    "locs = list(set([item for sublist in list(east_loc_dict.values()) for item in sublist])) + list(set([item for sublist in list(west_loc_dict.values()) for item in sublist])) + list(set([item for sublist in list(north_loc_dict.values()) for item in sublist])) + list(set([item for sublist in list(south_loc_dict.values()) for item in sublist])) + list(set([item for sublist in list(central_loc_dict.values()) for item in sublist]))\n",
    "emails = list(set([item for sublist in list(east_email_dict.values()) for item in sublist])) + list(set([item for sublist in list(west_email_dict.values()) for item in sublist])) + list(set([item for sublist in list(north_email_dict.values()) for item in sublist])) + list(set([item for sublist in list(south_email_dict.values()) for item in sublist])) + list(set([item for sublist in list(central_email_dict.values()) for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07520f7e-cbc6-4e72-8a0a-491e045eeb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_df = pd.read_csv(\"../data/others/us_cities_states_counties.csv\",  sep=\"|\", on_bad_lines='skip')\n",
    "\n",
    "locs =  set(location_df.City.to_list() + location_df['State full'].to_list())\n",
    "locs = set([str(loc).lower() for loc in locs])\n",
    "locs = sorted(locs, key=lambda x: (-len(x), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bf0842f-632a-461f-93e5-d7741e6c338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_dict, loc_dict, email_dict = ({} for i in range(3))\n",
    "\n",
    "for idx, name in enumerate(names):\n",
    "    if len(name) >= 1:\n",
    "        doc = nlp(name.lower())\n",
    "        if doc[0].pos_ in {\"PROPN\"}:\n",
    "            names_dict[name] = \"<PERSON_\" + str(idx) + \">\"\n",
    "    \n",
    "for idx, loc in enumerate(locs):\n",
    "    loc_dict[loc] = \"<LOCATION_\" + str(idx) + \">\"\n",
    "    \n",
    "for idx, email in enumerate(emails):\n",
    "    if len(email) >= 2:\n",
    "        email_dict[email] = \"<EMAIL_\" + str(idx) + \">\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b43c771e-32b1-48e3-be05-2dd534b391bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_df, all_loc_dict=loc_dict, all_name_dict=names_dict, all_email_dict=email_dict):\n",
    "    data_list = []\n",
    "    all_vendors = list(data_df.VENDOR.unique())\n",
    "    pbar = tqdm(total=len(all_vendors))\n",
    "    for vendor_id in all_vendors:\n",
    "        data = data_df[data_df.VENDOR == vendor_id]\n",
    "        # cleaning the text\n",
    "        data.TEXT = data.TEXT.apply(lambda x: anonymize_age(x))\n",
    "        data.TEXT = data.TEXT.apply(lambda x: anonymize_postid(x))\n",
    "        data.TEXT = data.TEXT.apply(lambda x: anonymize_emails(x, all_email_dict))\n",
    "        data.TEXT = data.TEXT.apply(lambda x: anonymize_links(x))\n",
    "        data.TEXT = data.TEXT.apply(lambda x: anonymize_numbers(x))\n",
    "        # data.TEXT = data.TEXT.apply(lambda x: anonymize_locations(x, all_loc_dict))\n",
    "        # data.TEXT = data.TEXT.apply(lambda x: anonymize_names(x, all_name_dict))\n",
    "        data.TEXT = data.TEXT.apply(lambda x: x.strip())\n",
    "        data_list.append(data)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    return pd.concat(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f006926b-b21c-4813-9c3e-bd3f92091d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "north_df = process_data(north_df)\n",
    "west_df = process_data(west_df)\n",
    "south_df = process_data(south_df)\n",
    "east_df = process_data(east_df)\n",
    "central_df = process_data(central_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2db6f329-47bb-48a2-b9a5-0fa693adaf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "north_df.to_csv(\"../data/processed/TEXT/north.csv\")\n",
    "west_df.to_csv(\"../data/processed/TEXT/west.csv\")\n",
    "south_df.to_csv(\"../data/processed/TEXT/south.csv\")\n",
    "east_df.to_csv(\"../data/processed/TEXT/east.csv\")\n",
    "central_df.to_csv(\"../data/processed/TEXT/central.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe490e09-d09f-4276-924a-d242fadabff8",
   "metadata": {},
   "source": [
    "# Generating heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "944e0142-f61b-4288-8e64-89acb9bcabae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12738/3909166935.py:13: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All phone numbers: ['814-602-6807']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_132.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_df = process_data(west_df, 63072)\n",
    "generate_heatmap(sample_df, 63072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bd73bbd6-fce4-4ef8-a087-a8d2e8cc6abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12738/3909166935.py:13: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All phone numbers: ['8145049661']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_133.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_df = process_data(west_df, 63076)\n",
    "generate_heatmap(sample_df, 63076)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "87b924da-9261-4cba-a2a5-aa5f722c345d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12738/3909166935.py:13: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All phone numbers: ['814 923 6153', '814 920 8940', '814 920 8940']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_134.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_df = process_data(west_df, 63081)\n",
    "generate_heatmap(sample_df, 63081)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c300e6-a4b8-4f9b-8a13-73e56da167e6",
   "metadata": {},
   "source": [
    "# Computing similarity between advertisements of 2 vendors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f3eaccd4-2ec1-4f45-9a8e-4def5a4083d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_between_vendor_ads(data_df1, vendor_id1, data_df2, vendor_id2):\n",
    "    sent_list1 = set(data_df1[data_df1.VENDOR==vendor_id1]['TEXT'].to_list())\n",
    "    sent_list2 = set(data_df2[data_df2.VENDOR==vendor_id2]['TEXT'].to_list())\n",
    "    outer_list = []\n",
    "    for index1, sent1 in enumerate(sent_list):\n",
    "        inner_list = []\n",
    "        for index2, sent2 in enumerate(sent_list):\n",
    "            emb1 = model.encode(sent1)\n",
    "            emb2 = model.encode(sent2)\n",
    "            inner_list.append(util.cos_sim(emb1, emb2).cpu().detach().numpy()[0][0])\n",
    "        outer_list.append(inner_list)\n",
    "\n",
    "    fig = px.imshow(outer_list, text_auto=True, aspect=\"auto\")\n",
    "    fig.show('iframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0850e037-1dec-439f-893e-867685bc150a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_138.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compute_similarity_between_vendor_ads(west_df, 63072, west_df, 63081)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "162a49f9-029c-4b7d-8f28-bb68da822a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_139.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compute_similarity_between_vendor_ads(west_df, 63081, west_df, 63076)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcf975a-c06e-4879-b856-8d24520e37f3",
   "metadata": {},
   "source": [
    "# Computing avg-similarity between advertisements of same vendors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e89986d7-1f55-4fc6-a471-b392fc1c9e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_for_vendors(data_df):\n",
    "    vendor_dict = {}\n",
    "    vendors = data_df.VENDOR.to_list()    \n",
    "    \n",
    "    pbar = tqdm(total=len(vendors))\n",
    "    for vendor_id in vendors:\n",
    "        sent_list = set(data_df[data_df.VENDOR==vendor_id]['TEXT'].to_list())\n",
    "        outer_list = []\n",
    "        for index1, sent1 in enumerate(sent_list):\n",
    "            inner_list = []\n",
    "            for index2, sent2 in enumerate(sent_list):\n",
    "                emb1 = model.encode(sent1)\n",
    "                emb2 = model.encode(sent2)\n",
    "                inner_list.append(util.cos_sim(emb1, emb2).cpu().detach().numpy()[0][0])\n",
    "            outer_list.append(inner_list)\n",
    "        vendor_dict[vendor_id] = np.array(outer_list).mean()\n",
    "        pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    return vendor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3bc106-a924-4543-bb6d-b048075b4f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "east_dict = compute_similarity_for_vendors(east_df)\n",
    "west_dict = compute_similarity_for_vendors(west_df)\n",
    "north_dict = compute_similarity_for_vendors(north_df)\n",
    "south_dict = compute_similarity_for_vendors(south_df)\n",
    "central_dict = compute_similarity_for_vendors(central_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e20190-93a0-4b08-9a1a-896f368b3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "pathlib.Path('../pickled/').mkdir(parents=True, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5267c42d-8490-4b97-95fb-bae5e56ee936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../pickled/east_ads_similarity_per_vendor_zero_shot.pickle', 'wb') as handle:\n",
    "    pickle.dump(east_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../pickled/west_ads_similarity_per_vendor_zero_shot.pickle', 'wb') as handle:\n",
    "    pickle.dump(west_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../pickled/north_ads_similarity_per_vendor_zero_shot.pickle', 'wb') as handle:\n",
    "    pickle.dump(north_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../pickled/south_ads_similarity_per_vendor_zero_shot.pickle', 'wb') as handle:\n",
    "    pickle.dump(south_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../pickled/central_ads_similarity_per_vendor_zero_shot.pickle', 'wb') as handle:\n",
    "    pickle.dump(central_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a3637-6026-45db-b5b3-88179efb3430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HT",
   "language": "python",
   "name": "ht"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
