{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b763ea-5fee-4365-a302-ac18e222fba8",
   "metadata": {},
   "source": [
    "# Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bf364b2-844d-45c6-be2c-4433d66aef0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/HT/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %% Importing Libraries\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import argparse\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, random_split, TensorDataset\n",
    "\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import lightning as L\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.tuner.tuning import Tuner\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.strategies import DeepSpeedStrategy\n",
    "from lightning.pytorch.plugins.precision import DeepSpeedPrecisionPlugin\n",
    "\n",
    "from deepspeed.ops.adam import DeepSpeedCPUAdam\n",
    "\n",
    "import t5_encoder\n",
    "\n",
    "# Custom library\n",
    "sys.path.append('../process/')\n",
    "from loadData import HTClassifierDataModule\n",
    "\n",
    "sys.path.append('../architectures/')\n",
    "from HTClassifier import HTClassifierModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a60260-9e88-42dc-9d69-6eb9331883d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating directories if they don't exist\n",
    "Path('../pickled/embeddings').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8616e036-1329-4e9a-acdd-63545c72bfb5",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4acb4f77-05f1-4c1c-bc91-7121accd930e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1111"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.model_name_or_path = 'johngiorgi/declutr-small'\n",
    "        self.tokenizer_name_or_path = 'johngiorgi/declutr-small'\n",
    "        self.data_dir = \"../data/processed/TEXT/\"\n",
    "        self.demography = \"merged\"\n",
    "        self.temp = 0.07 # Temperature for softmax\n",
    "        self.max_seq_length = 512\n",
    "        self.learning_rate = 3e-5 \n",
    "        self.adam_epsilon = 1e-6\n",
    "        self.warmup_steps = 0\n",
    "        self.dropout = 0.3\n",
    "        self.weight_decay = 0.01\n",
    "        self.num_train_epochs = 1\n",
    "        self.gradient_accumulation_steps = 4\n",
    "        self.pad_to_max_length = True\n",
    "        self.batch_size = 32\n",
    "        self.output_dir = '../models/text-classifier-baselines/'\n",
    "        self.overwrite = True\n",
    "        self.local_rank = -1\n",
    "        self.no_cuda = False\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "seed_everything(1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ac4e4e7-731b-4a95-9c4a-812135ffd7f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../process/loadData.py:179: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data_df = pd.read_csv(os.path.join(self.args.data_dir, self.args.demography + '.csv'), error_bad_lines=False, warn_bad_lines=False)\n",
      "../process/loadData.py:179: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data_df = pd.read_csv(os.path.join(self.args.data_dir, self.args.demography + '.csv'), error_bad_lines=False, warn_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "dm = HTClassifierDataModule(args)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fca23b53-50f5-498d-b93d-448444a7797d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38874/4083842773.py:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  args.num_classes = pd.read_csv(os.path.join(args.data_dir, args.demography + '.csv'), error_bad_lines=False, warn_bad_lines=False).VENDOR.nunique()\n",
      "/tmp/ipykernel_38874/4083842773.py:1: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  args.num_classes = pd.read_csv(os.path.join(args.data_dir, args.demography + '.csv'), error_bad_lines=False, warn_bad_lines=False).VENDOR.nunique()\n"
     ]
    }
   ],
   "source": [
    "args.num_classes = pd.read_csv(os.path.join(args.data_dir, args.demography + '.csv'), error_bad_lines=False, warn_bad_lines=False).VENDOR.nunique()\n",
    "\n",
    "args.num_training_steps = len(dm.train_dataloader()) * 2\n",
    "# Setting the warmup steps to 1/10th the size of training data\n",
    "args.warmup_steps = int(len(dm.train_dataloader()) * 10/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb7837-74a6-4b4d-b87d-220879c95439",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63a3ecb5-4743-4d75-9780-9431c71f02f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HTClassifierModel(pl.LightningModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        if isinstance(args, tuple) and len(args) > 0: \n",
    "            self.args = args[0]\n",
    "            self.hparams.learning_rate = self.args.learning_rate\n",
    "            self.hparams.eps = self.args.adam_epsilon\n",
    "            self.hparams.weight_decay = self.args.weight_decay\n",
    "            self.hparams.model_name_or_path = self.args.model_name_or_path\n",
    "            self.hparams.num_classes = self.args.num_classes\n",
    "            self.hparams.num_training_steps = self.args.num_training_steps\n",
    "            self.hparams.warmup_steps = self.args.warmup_steps\n",
    "        \n",
    "        # freeze\n",
    "        self._frozen = False\n",
    "\n",
    "        # Handling the padding token in distilgpt2 by substituting it with eos_token_id\n",
    "        if self.hparams.model_name_or_path == \"distilgpt2\":\n",
    "            config = AutoConfig.from_pretrained(self.hparams.model_name_or_path, num_labels=self.hparams.num_classes, output_attentions=True, output_hidden_states=True)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(self.hparams.model_name_or_path, config=config)\n",
    "            self.model.config.pad_token_id = self.model.config.eos_token_id\n",
    "        else:\n",
    "            config = AutoConfig.from_pretrained(self.hparams.model_name_or_path, num_labels=self.hparams.num_classes, output_attentions=True, output_hidden_states=True)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(self.hparams.model_name_or_path, config=config)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # The batch contains the input_ids, the input_put_mask and the labels (for training)\n",
    "        input_ids = batch[0]\n",
    "        input_mask = batch[1]\n",
    "        labels = batch[2]\n",
    "\n",
    "        outputs = self.model(input_ids, attention_mask=input_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "\n",
    "        return outputs, loss, logits\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # the training step is a (virtual) method,specified in the interface, that the pl.LightningModule\n",
    "        # class stipulates you to overwrite. This we do here, by virtue of this definition\n",
    "        outputs = self(batch)  # self refers to the model, which in turn acceses the forward method\n",
    "        train_loss = outputs[0]\n",
    "        self.log_dict({\"train_loss\": train_loss, \"learning_rate\":self.hparams.learning_rate}, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return train_loss\n",
    "        # the training_step method expects a dictionary, which should at least contain the loss\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        # the training step is a (virtual) method,specified in the interface, that the pl.LightningModule\n",
    "        # class  wants you to overwrite, in case you want to do validation. This we do here, by virtue of this definition.\n",
    "\n",
    "        outputs = self(batch)\n",
    "        # self refers to the model, which in turn accesses the forward method\n",
    "\n",
    "        # Apart from the validation loss, we also want to track validation accuracy  to get an idea, what the\n",
    "        # model training has achieved \"in real terms\".\n",
    "        val_loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        labels = batch[2]\n",
    "\n",
    "        # Evaluating the performance\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        balanced_accuracy = balanced_accuracy_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), adjusted=True)\n",
    "        macro_accuracy = f1_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), average='macro')\n",
    "        micro_accuracy = f1_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), average='micro')\n",
    "        weighted_accuracy = f1_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), average='weighted')\n",
    "        \n",
    "        self.log_dict({\"val_loss\": val_loss, 'accuracy': balanced_accuracy, 'macro-F1': macro_accuracy, 'micro-F1': micro_accuracy, 'weighted-F1':weighted_accuracy}, \n",
    "                      on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return val_loss\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        # the training step is a (virtual) method,specified in the interface, that the pl.LightningModule\n",
    "        # class  wants you to overwrite, in case you want to do test. This we do here, by virtue of this definition.\n",
    "\n",
    "        outputs = self(batch)\n",
    "        # self refers to the model, which in turn accesses the forward method\n",
    "\n",
    "        # Apart from the validation loss, we also want to track validation accuracy  to get an idea, what the\n",
    "        # model training has achieved \"in real terms\".\n",
    "        test_loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        labels = batch[2]\n",
    "\n",
    "        # Evaluating the performance\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        balanced_accuracy = balanced_accuracy_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), adjusted=True)\n",
    "        macro_accuracy = f1_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), average='macro')\n",
    "        micro_accuracy = f1_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), average='micro')\n",
    "        weighted_accuracy = f1_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), average='weighted')\n",
    "        \n",
    "        self.log_dict({\"test_loss\": test_loss, 'accuracy': balanced_accuracy, 'macro-F1': macro_accuracy, 'micro-F1': micro_accuracy, 'weighted-F1':weighted_accuracy}, \n",
    "                      on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "    \n",
    "    def predict_step(self, batch, batch_nb):\n",
    "        # the training step is a (virtual) method,specified in the interface, that the pl.LightningModule\n",
    "        # class  wants you to overwrite, in case you want to do validation. This we do here, by virtue of this definition.\n",
    "\n",
    "        outputs = self(batch)\n",
    "        # self refers to the model, which in turn accesses the forward method\n",
    "\n",
    "        # Apart from the validation loss, we also want to track validation accuracy  to get an idea, what the\n",
    "        # model training has achieved \"in real terms\".\n",
    "        val_loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        labels = batch[2]\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        return predictions.detach().cpu().numpy()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # The configure_optimizers is a (virtual) method, specified in the interface, that the\n",
    "        # pl.LightningModule class wants you to overwrite.\n",
    "\n",
    "        # In this case we define that some parameters are optimized in a different way than others. In\n",
    "        # particular we single out parameters that have 'bias', 'LayerNorm.weight' in their names. For those\n",
    "        # we do not use an optimization technique called weight decay.\n",
    "\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "        optimizer_grouped_parameters = [{'params': [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay':self.hparams.weight_decay}, \n",
    "                                        {'params': [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "        # optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.eps)\n",
    "        optimizer = DeepSpeedCPUAdam(optimizer_grouped_parameters, adamw_mode=True, lr=self.hparams.learning_rate, betas=(0.9, 0.999), eps=self.hparams.eps)\n",
    "\n",
    "        # We also use a scheduler that is supplied by transformers.\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=self.hparams.num_training_steps)\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def freeze(self) -> None:\n",
    "        # freeze all layers, except the final classifier layers\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'classifier' not in name:  # classifier layer\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self._frozen = True\n",
    "\n",
    "    def unfreeze(self) -> None:\n",
    "        if self._frozen:\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if 'classifier' not in name:  # classifier layer\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        self._frozen = False\n",
    "\n",
    "    def train_epoch_start(self):\n",
    "        \"\"\"pytorch lightning hook\"\"\"\n",
    "        if self.current_epoch < self.hparams.nr_frozen_epochs:\n",
    "            self.freeze()\n",
    "\n",
    "        if self.current_epoch >= self.hparams.nr_frozen_epochs:\n",
    "            self.unfreeze() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "200f58d4-56ad-4527-8ba9-77bf641c59f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at johngiorgi/declutr-small were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at johngiorgi/declutr-small and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at johngiorgi/declutr-small were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at johngiorgi/declutr-small and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = HTClassifierModel(args).load_from_checkpoint(\"/workspace/persistent/human-trafficking/models/text-classifier-baselines/seed:1111/merged/declutr-small/final_model_new.pt\").eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3f9076-e916-489e-9eed-14397552fc6f",
   "metadata": {},
   "source": [
    "# Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "697ffe6b-2644-4679-86ba-1dc0dd19be11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed/TEXT/merged.csv\")\n",
    "\n",
    "text = df.TEXT.values.tolist()\n",
    "vendors = df.VENDOR.values.tolist()\n",
    "\n",
    "# Since the vendor IDs are not the current representations of the class labels, we remap these label IDs to avoid falling into out-of-bounds problem\n",
    "vendors_dict = {}\n",
    "i = 0\n",
    "for vendor in vendors:\n",
    "    if vendor not in vendors_dict.keys():\n",
    "        vendors_dict[vendor] = i\n",
    "        i += 1\n",
    "        \n",
    "train_df, test_df = train_test_split(df, test_size=0.20, random_state=1111)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10190575-3c67-4cb1-8985-3364ea359bd0",
   "metadata": {},
   "source": [
    "# Extracting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efcad799-ba4e-4594-b38d-ddcc692f0af5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d92543ca-b685-485a-9b8b-ca07ed232d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85b11eb3-dae8-4980-9446-83954d69ea90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_representations(test_df, demo, vendors_dict, pooling_type=\"mean\", device=\"cpu\", batch_size=32):\n",
    "    # data_test = test_df[test_df.DEMO==demo]\n",
    "    data_test = test_df\n",
    "    data_test.replace({\"VENDOR\": vendors_dict}, inplace=True)\n",
    "\n",
    "    text = data_test.TEXT.values.tolist()\n",
    "    vendors = data_test.VENDOR.values.tolist()\n",
    "\n",
    "    # Tokenizing the data with padding and truncation\n",
    "    encodings = tokenizer(text, add_special_tokens=True, max_length=512, padding='max_length', return_token_type_ids=False, truncation=True, \n",
    "                               return_attention_mask=True, return_tensors='pt') \n",
    "\n",
    "    # Move the encodings to the device\n",
    "    input_ids = encodings['input_ids'].to(device)\n",
    "    attention_mask = encodings['attention_mask'].to(device)\n",
    "    labels = torch.tensor(vendors).to(device)\n",
    "\n",
    "    # Combine the inputs into a TensorDataset.\n",
    "    dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "    test_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    pooled_output_list, labels_list = [], []\n",
    "    \n",
    "    pbar = tqdm(total=len(test_dataloader))\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            attention_mask = batch[1]\n",
    "            labels = batch[2]\n",
    "\n",
    "            outputs = model(batch)\n",
    "\n",
    "            # Extracting the output from last hidden state\n",
    "            hidden_states = torch.stack(outputs[0][2])[-1]\n",
    "\n",
    "            # Generating the pooled output\n",
    "            if pooling_type == \"mean\":\n",
    "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "                sum_embeddings = torch.sum(hidden_states * input_mask_expanded, 1)\n",
    "                sum_mask = input_mask_expanded.sum(1)\n",
    "                sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "                pooled_output = sum_embeddings / sum_mask\n",
    "            elif pooling_type == \"max\":\n",
    "                last_hidden_state = hidden_states\n",
    "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "                last_hidden_state[input_mask_expanded == 0] = float(\"-inf\")  # Set padding tokens to large negative value\n",
    "                pooled_output = torch.max(last_hidden_state, 1)[0]\n",
    "            else:\n",
    "                # Mean-max pooling\n",
    "                last_hidden_state = hidden_states\n",
    "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "                sum_embeddings = torch.sum(hidden_states * input_mask_expanded, 1)\n",
    "                sum_mask = input_mask_expanded.sum(1)\n",
    "                sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "                mean_pooled_output = sum_embeddings / sum_mask\n",
    "                last_hidden_state[input_mask_expanded == 0] = float(\"-inf\")  # Set padding tokens to large negative value\n",
    "                max_pooled_output = torch.max(last_hidden_state, 1)[0]\n",
    "                pooled_output = torch.cat((mean_pooled_output, max_pooled_output), 1)\n",
    "\n",
    "            pooled_output_list.append(pooled_output)\n",
    "            labels_list.append(labels)\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "    # Concatenate the pooled outputs and labels into tensors\n",
    "    pooled_outputs = torch.cat(pooled_output_list)\n",
    "    labels = torch.cat(labels_list)\n",
    "\n",
    "    return pooled_outputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8b6a060-3bd5-4fe6-a4b3-b9cb0c3fb3b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2190/2190 [3:05:21<00:00,  5.08s/it]  \n"
     ]
    }
   ],
   "source": [
    "for pooling in [\"mean\", \"max\", \"mean-max\"]:\n",
    "    pooled_outputs, labels = extract_representations(train_df, \"merged\", vendors_dict, pooling_type=pooling)\n",
    "    pooled_output_filename = \"trained_traindata_declutr_\" + pooling + \".pt\"\n",
    "    labels_filename = \"trained_trainlabels_declutr_\" + pooling + \".pt\"\n",
    "\n",
    "    torch.save(pooled_outputs, os.path.join(os.getcwd(), \"../pickled/embeddings\", pooled_output_filename))\n",
    "    torch.save(labels, os.path.join(os.getcwd(), \"../pickled/embeddings\", labels_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef5a829b-f461-4841-b47b-4256bf9ff340",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 548/548 [47:17<00:00,  5.18s/it]\n"
     ]
    }
   ],
   "source": [
    "for pooling in [\"mean\", \"max\", \"mean-max\"]:\n",
    "    pooled_outputs, labels = extract_representations(test_df, \"merged\", vendors_dict, pooling_type=pooling)\n",
    "    pooled_output_filename = \"trained_testdata_declutr_\" + pooling + \".pt\"\n",
    "    labels_filename = \"trained_testlabels_declutr_\" + pooling + \".pt\"\n",
    "\n",
    "    torch.save(pooled_outputs, os.path.join(os.getcwd(), \"../pickled/embeddings\", pooled_output_filename))\n",
    "    torch.save(labels, os.path.join(os.getcwd(), \"../pickled/embeddings\", labels_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac27ed08-5636-4819-916f-16167c273c5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading the embeddings from an un-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a26cf6da-3818-4605-96ae-99e8697362cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)95525/.gitattributes: 100%|██████████| 1.17k/1.17k [00:00<00:00, 80.9kB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 67.8kB/s]\n",
      "Downloading (…)ed27695525/README.md: 100%|██████████| 3.96k/3.96k [00:00<00:00, 595kB/s]\n",
      "Downloading (…)27695525/config.json: 100%|██████████| 718/718 [00:00<00:00, 286kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 117/117 [00:00<00:00, 42.5kB/s]\n",
      "Downloading (…)aluation_results.csv: 100%|██████████| 659/659 [00:00<00:00, 245kB/s]\n",
      "Downloading (…)d27695525/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.86MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 499M/499M [00:15<00:00, 31.7MB/s] \n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 20.7kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 100kB/s]\n",
      "Downloading (…)95525/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 8.50MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 354/354 [00:00<00:00, 144kB/s]\n",
      "Downloading (…)d27695525/vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 2.44MB/s]\n",
      "Downloading (…)7695525/modules.json: 100%|██████████| 229/229 [00:00<00:00, 102kB/s]\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer(args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49ed1bac-fbfc-4168-a15b-99c562ae4afb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed/TEXT/merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "805b69f7-7d14-4ba3-8a0d-90256673fe59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.20, random_state=1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf6d2f0-e296-4434-be66-40657542a255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = model.encode(train_df[\"TEXT\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2abf91-15b8-4368-818f-70d69ba93666",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = torch.tensor(train_df.VENDOR.to_list())\n",
    "labels.shape, embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136ba5df-0703-4a36-ab18-ce4ff02236af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(embeddings, os.path.join(os.getcwd(), \"../pickled/embeddings\", \"untrained_styledata_train.pt\"))\n",
    "torch.save(labels, os.path.join(os.getcwd(), \"../pickled/embeddings\", \"untrained_stylelabels_train.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20445a4e-a3d7-42af-ad70-1bb5dd619898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = model.encode(test_df[\"TEXT\"].to_list())\n",
    "labels = torch.tensor(test_df.VENDOR.to_list())\n",
    "labels.shape, embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0fad60-abb4-4d3b-b49d-68879921c0ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(embeddings, os.path.join(os.getcwd(), \"../pickled/embeddings\", \"untrained_styledata_test.pt\"))\n",
    "torch.save(labels, os.path.join(os.getcwd(), \"../pickled/embeddings\", \"untrained_stylelabels_test.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90f354b-6e64-42b4-a8d0-0c7b9333de9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
